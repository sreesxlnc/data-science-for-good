{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "a11a4a9a92130faf0f55658f580d3507d4988b47"
   },
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# Clearing up memory\n",
    "import gc\n",
    "\n",
    "# Modeling\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "# Bayesian Hyperparameter optimization\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "\n",
    "# Utilities\n",
    "import csv\n",
    "import json\n",
    "import ast\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "9beddb3fc943345f5cdc84d160e2033c5d9931eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willk\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (572) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('data/ft_2000_important.csv')\n",
    "submit_base = pd.read_csv('data/test.csv')[['Id', 'idhogar']]\n",
    "\n",
    "train = features[features['Target'].notnull()].copy()\n",
    "test = features[features['Target'].isnull()].copy()\n",
    "\n",
    "train_labels = np.array(train.pop('Target'))\n",
    "test_ids = list(test.pop('idhogar'))\n",
    "\n",
    "train, test = train.align(test, join = 'inner', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "5ac0fce32cf415c953af2f57b1058d90c2200fc9"
   },
   "outputs": [],
   "source": [
    "for c in train:\n",
    "    if train[c].dtype == 'object':\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "484aef46374b5e7616af2d51a562f1111cace6f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train objects:  Index([], dtype='object')\n",
      "Test objects:  Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Train objects: ', train.columns[np.where(train.dtypes == 'object')])\n",
    "print('Test objects: ', test.columns[np.where(test.dtypes == 'object')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "810b829cdbfd1efa0bee02c063d01a627e5d0d5a"
   },
   "outputs": [],
   "source": [
    "# missing_threshold = 0.95\n",
    "# correlation_threshold = 0.99\n",
    "\n",
    "# feature_matrix = feature_matrix.replace({np.inf: np.nan, -np.inf:np.nan})\n",
    "\n",
    "# # One hot encoding (if necessary)\n",
    "# feature_matrix = pd.get_dummies(feature_matrix)\n",
    "# n_features_start = feature_matrix.shape[1]\n",
    "# print('Original shape: ', feature_matrix.shape)\n",
    "\n",
    "# # Find missing and percentage\n",
    "# missing = pd.DataFrame(feature_matrix.isnull().sum())\n",
    "# missing['fraction'] = missing[0] / feature_matrix.shape[0]\n",
    "# missing.sort_values('fraction', ascending = False, inplace = True)\n",
    "\n",
    "# # Missing above threshold\n",
    "# missing_cols = list(missing[missing['fraction'] > missing_threshold].index)\n",
    "# n_missing_cols = len(missing_cols)\n",
    "\n",
    "# # Remove missing columns\n",
    "# feature_matrix = feature_matrix[[x for x in feature_matrix if x not in missing_cols]]\n",
    "# print('{} missing columns with threshold: {}.'.format(n_missing_cols, missing_threshold))\n",
    "\n",
    "# # Zero variance\n",
    "# unique_counts = pd.DataFrame(feature_matrix.nunique()).sort_values(0, ascending = True)\n",
    "# zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n",
    "# n_zero_variance_cols = len(zero_variance_cols)\n",
    "\n",
    "# # Remove zero variance columns\n",
    "# feature_matrix = feature_matrix[[x for x in feature_matrix if x not in zero_variance_cols]]\n",
    "# print('{} zero variance columns.'.format(n_zero_variance_cols))\n",
    "\n",
    "# # Correlations\n",
    "# corr_matrix = feature_matrix.corr()\n",
    "\n",
    "# # Extract the upper triangle of the correlation matrix\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# # Select the features with correlations above the threshold\n",
    "# # Need to use the absolute value\n",
    "# to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "# n_collinear = len(to_drop)\n",
    "\n",
    "# feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]\n",
    "# print('{} collinear columns removed with correlation above {}.'.format(n_collinear,  correlation_threshold))\n",
    "\n",
    "# total_removed = n_missing_cols + n_zero_variance_cols + n_collinear\n",
    "\n",
    "# print('Total columns removed: ', total_removed)\n",
    "# print('Shape after feature selection: {}.'.format(feature_matrix.shape))\n",
    "\n",
    "# # Remove columns derived from the Target\n",
    "# drop_cols = []\n",
    "# for col in feature_matrix:\n",
    "#     if col == 'Target':\n",
    "#         pass\n",
    "#     else:\n",
    "#         if 'Target' in col:\n",
    "#             drop_cols.append(col)\n",
    "\n",
    "# feature_matrix = feature_matrix[[x for x in feature_matrix if x not in drop_cols]]    \n",
    "\n",
    "# # Extract out training and testing data\n",
    "# train = feature_matrix[feature_matrix['Target'].notnull()]\n",
    "# test = feature_matrix[feature_matrix['Target'].isnull()]\n",
    "\n",
    "# train_labels = np.array(train.pop('Target')).reshape((-1, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c38c615a6051390d64abb1f4dddbec7e0dc8fd01"
   },
   "source": [
    "## Custom Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "4e2915cc5e7f5779a1fb2f6279b0023b0d12717c"
   },
   "outputs": [],
   "source": [
    "def macro_f1_score(labels, predictions):\n",
    "    # Reshape the predictions as needed\n",
    "    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n",
    "    \n",
    "    metric_value = f1_score(labels, predictions, average = 'macro')\n",
    "    \n",
    "    # Return is name, value, is_higher_better\n",
    "    return 'macro_f1', metric_value, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0804fd22c0bc846fb3e46c565b7fc47a2880c57"
   },
   "source": [
    "# Objective Function to Minimize (F1 Cross Validation Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "02f600358af2a4d2f3c361df7f409cd22eea92ad"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "cf5abd0f0ddb92258a198f2d364e91a8108d4880"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'215'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f'{43 * 5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "f64aaab5660e0d972ac9793d748ecba1702bbbd5"
   },
   "outputs": [],
   "source": [
    "def objective(hyperparameters, nfolds=5):\n",
    "    \"\"\"Return validation score from hyperparameters for LightGBM\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    \n",
    "    # Retrieve the subsample\n",
    "    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "    # Extract the boosting type and subsample to top level keys\n",
    "    hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']\n",
    "    hyperparameters['subsample'] = subsample\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n",
    "\n",
    "    if 'n_estimators' in hyperparameters:\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "    # Using stratified kfold cross validation\n",
    "    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n",
    "    \n",
    "    # Convert to arrays for indexing\n",
    "    features = np.array(train)\n",
    "    labels = np.array(train_labels).reshape((-1 ))\n",
    "    \n",
    "    valid_scores = []\n",
    "    best_estimators = []\n",
    "    run_times = []\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**hyperparameters, class_weight = 'balanced',\n",
    "                               n_jobs=-1, metric = 'None',\n",
    "                               n_estimators=10000)\n",
    "    \n",
    "    # Iterate through the folds\n",
    "    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n",
    "        \n",
    "        # Training and validation data\n",
    "        X_train = features[train_indices]\n",
    "        X_valid = features[valid_indices]\n",
    "        y_train = labels[train_indices]\n",
    "        y_valid = labels[valid_indices]\n",
    "        \n",
    "        start = timer()\n",
    "        # Train with early stopping\n",
    "        model.fit(X_train, y_train, early_stopping_rounds = 100, \n",
    "                  eval_metric = macro_f1_score, \n",
    "                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n",
    "                  eval_names = ['train', 'valid'],\n",
    "                  verbose = 400)\n",
    "        end = timer()\n",
    "        # Record the validation fold score\n",
    "        valid_scores.append(model.best_score_['valid']['macro_f1'])\n",
    "        best_estimators.append(model.best_iteration_)\n",
    "        \n",
    "        run_times.append(end - start)\n",
    "    \n",
    "    score = np.mean(valid_scores)\n",
    "    score_std = np.std(valid_scores)\n",
    "    loss = 1 - score\n",
    "    \n",
    "    run_time = np.mean(run_times)\n",
    "    run_time_std = np.std(run_times)\n",
    "    \n",
    "    estimators = int(np.mean(best_estimators))\n",
    "    hyperparameters['n_estimators'] = estimators\n",
    "    \n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(OUT_FILE, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, hyperparameters, ITERATION, run_time, score, score_std])\n",
    "    of_connection.close()\n",
    "    \n",
    "    # Display progress\n",
    "    if ITERATION % PROGRESS == 0:\n",
    "        display(f'Iteration: {ITERATION}, Current Score: {round(score, 4)}.')\n",
    "    \n",
    "    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n",
    "            'time': run_time, 'time_std': run_time_std, 'status': STATUS_OK, \n",
    "            'score': score, 'score_std': score_std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "faab04de2a932f79fb32b0bb606e02221a462213"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Search Domain\n",
    "\"\"\"\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'boosting_type': hp.choice('boosting_type', \n",
    "                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                             {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                             {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 3, 50, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.025), np.log(0.25)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 2000, 100000, 2000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 5, 80, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.5, 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb570d118401b66eef9f291d54d5a791876b7fba"
   },
   "source": [
    "## Methods for Recording Results\n",
    "\n",
    "* Trials object\n",
    "* Writing to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "1a0091c9706ce833cd0df56f29c59af2613d8cb8"
   },
   "outputs": [],
   "source": [
    "# Record results\n",
    "trials = Trials()\n",
    "\n",
    "# Create a file and open a connection\n",
    "OUT_FILE = 'optimization3.csv'\n",
    "of_connection = open(OUT_FILE, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "MAX_EVALS = 100\n",
    "PROGRESS = 10\n",
    "N_FOLDS = 5\n",
    "ITERATION = 0\n",
    "\n",
    "# Write column names\n",
    "headers = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score', 'std']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "6d6036a2ce320eeb6371327affa5034d3b0feb28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Running Optimization for 100 Trials.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Iteration: 10, Current Score: 0.4444.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Iteration: 20, Current Score: 0.425.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Iteration: 30, Current Score: 0.4373.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Iteration: 40, Current Score: 0.4478.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Iteration: 50, Current Score: 0.4259.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Iteration: 60, Current Score: 0.4262.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Iteration: 70, Current Score: 0.433.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Iteration: 80, Current Score: 0.4419.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Iteration: 90, Current Score: 0.441.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Iteration: 100, Current Score: 0.4392.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "display(\"Running Optimization for {} Trials.\".format(MAX_EVALS))\n",
    "\n",
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n",
    "            max_evals = MAX_EVALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "43b13e5e3035a15ccefdac8bd1b8c1ac1fbdfc22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 1, 'colsample_by_tree': 0.9123205582882894, 'dart_subsample': 0.6016593237412641, 'learning_rate': 0.24688968283162896, 'min_child_samples': 60.0, 'num_leaves': 22.0, 'reg_alpha': 0.5117491903285334, 'reg_lambda': 0.5627049996653902, 'subsample_for_bin': 70000.0}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save the trial results\n",
    "with open('trials.json', 'w') as f:\n",
    "    f.write(json.dumps(str(trials)))\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "629a839f8f94f26ddbee053ef856f4f89551a64c"
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv(OUT_FILE)\n",
    "results = results.sort_values('score', ascending = False).reset_index()\n",
    "best_hyp = ast.literal_eval(results.loc[0, 'hyperparameters'])\n",
    "results.to_csv('sorted_optimization3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'dart',\n",
       " 'colsample_bytree': 0.9123205582882894,\n",
       " 'learning_rate': 0.24688968283162896,\n",
       " 'min_child_samples': 60,\n",
       " 'n_estimators': 78,\n",
       " 'num_leaves': 22,\n",
       " 'reg_alpha': 0.5117491903285334,\n",
       " 'reg_lambda': 0.5627049996653902,\n",
       " 'subsample': 0.6016593237412641,\n",
       " 'subsample_for_bin': 70000}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "9ed053cd2d0d85cfdebe6b5b3180a40f6b1fdbdc"
   },
   "outputs": [],
   "source": [
    "del best_hyp['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "353754dfd38c293be66b6effde8831834032b744",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_gbm(hyperparameters, features, labels, \n",
    "              test_features, test_ids, submission_base, \n",
    "              nfolds = 5, return_preds = False):\n",
    "    \"\"\"Model using the GBM and cross validation.\n",
    "       Trains with early stopping on each fold.\n",
    "       Hyperparameters probably need to be tuned.\"\"\"\n",
    "    \n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Model with hyperparameters selected from previous work\n",
    "#     model = lgb.LGBMClassifier(boosting_type = 'gbdt', n_estimators = 10000, max_depth = -1,\n",
    "#                                learning_rate = 0.025, metric = 'None', min_child_samples = 30,\n",
    "#                                reg_alpha = 0.35, reg_lambda = 0.6, num_leaves = 15, \n",
    "#                                colsample_bytree = 0.85, objective = 'multiclass', \n",
    "#                                class_weight = 'balanced', \n",
    "#                                n_jobs = -1)\n",
    "\n",
    "    model = lgb.LGBMClassifier(**hyperparameters, \n",
    "                               objective = 'multiclass', n_jobs = -1, \n",
    "                               n_estimators = 10000, metric = 'None')\n",
    "    \n",
    "    # Using stratified kfold cross validation\n",
    "    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n",
    "    predictions = pd.DataFrame()\n",
    "    importances = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Convert to arrays for indexing\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    labels = np.array(labels).reshape((-1 ))\n",
    "    \n",
    "    valid_scores = []\n",
    "    \n",
    "    # Iterate through the folds\n",
    "    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n",
    "        # Dataframe for \n",
    "        fold_predictions = pd.DataFrame()\n",
    "        \n",
    "        # Training and validation data\n",
    "        X_train = features[train_indices]\n",
    "        X_valid = features[valid_indices]\n",
    "        y_train = labels[train_indices]\n",
    "        y_valid = labels[valid_indices]\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(X_train, y_train, early_stopping_rounds = 100, \n",
    "                  eval_metric = macro_f1_score,\n",
    "                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n",
    "                  eval_names = ['train', 'valid'],\n",
    "                  verbose = 200)\n",
    "        \n",
    "        # Record the validation fold score\n",
    "        valid_scores.append(model.best_score_['valid']['macro_f1'])\n",
    "        \n",
    "        # Make predictions from the fold\n",
    "        fold_probabilitites = model.predict_proba(test_features)\n",
    "        \n",
    "        # Record each prediction for each class as a column\n",
    "        for j in range(4):\n",
    "            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n",
    "            \n",
    "        fold_predictions['idhogar'] = test_ids\n",
    "        fold_predictions['fold'] = (i+1)\n",
    "        predictions = predictions.append(fold_predictions)\n",
    "        \n",
    "        importances += model.feature_importances_ / nfolds   \n",
    "        \n",
    "        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n",
    "\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names,\n",
    "                                        'importance': importances})\n",
    "    valid_scores = np.array(valid_scores)\n",
    "    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n",
    "    \n",
    "    # If we want to examine predictions don't average over folds\n",
    "    if return_preds:\n",
    "        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "        return predictions, feature_importances\n",
    "    \n",
    "    # Average the predictions over folds\n",
    "    predictions = predictions.groupby('idhogar', as_index = False).mean()\n",
    "    \n",
    "    # Find the class and associated probability\n",
    "    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "    predictions = predictions.drop(columns = ['fold'])\n",
    "    \n",
    "    # Merge with the base to have one prediction for each individual\n",
    "    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n",
    "        \n",
    "    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n",
    "    \n",
    "    # return the submission and feature importances\n",
    "    return submission, feature_importances, valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3393b2e644b4512da6ee00f3705b08243cb14aa3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "submission, feature_importances, valid_scores = model_gbm(best_hyp, train, train_labels,\n",
    "                                                          test, test_ids, submit_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3fb4f832181b241918d933985f2078e9a125ffe6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, n = 15, return_features = False, threshold = None):\n",
    "    \"\"\"Plots n most important features. Also plots the cumulative importance if\n",
    "    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n",
    "    Intended for use with any tree-based feature importances. \n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n",
    "    \n",
    "        n (int): Number of most important features to plot. Default is 15.\n",
    "    \n",
    "        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n",
    "        \n",
    "    Returns:\n",
    "        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n",
    "                        and a cumulative importance column\n",
    "    \n",
    "    Note:\n",
    "    \n",
    "        * Normalization in this case means sums to 1. \n",
    "        * Cumulative importance is calculated by summing features from most to least important\n",
    "        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort features with most important at the head\n",
    "    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "    \n",
    "    # Normalize the feature importances to add up to one and calculate cumulative importance\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "    \n",
    "    plt.rcParams['font.size'] = 12\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    # Bar plot of n most important features\n",
    "    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n",
    "                            x = 'feature', color = 'red', \n",
    "                            edgecolor = 'k', figsize = (12, 8),\n",
    "                            legend = False, linewidth = 2)\n",
    "\n",
    "    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n",
    "    plt.title(f'Top {n} Most Important Features', size = 18)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    \n",
    "    if threshold:\n",
    "        # Cumulative importance plot\n",
    "        plt.figure(figsize = (8, 6))\n",
    "        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n",
    "        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n",
    "        plt.title('Cumulative Feature Importance', size = 18);\n",
    "        \n",
    "        # Number of features needed for threshold cumulative importance\n",
    "        # This is the index (will need to add 1 for the actual number)\n",
    "        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "        \n",
    "        # Add vertical line to plot\n",
    "        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n",
    "        plt.show();\n",
    "        \n",
    "        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n",
    "                                                                                  100 * threshold))\n",
    "    if return_features:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b847c81d1e6ad3ea0a83d5af1a72b85c8472665f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(feature_importances, threshold = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc83b3aacb1c70bd07b608f7607b30240a3f5e21",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('bayesian_optimization_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7785dcf27d353d1596ccb87f773178d41a5fe2d",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
